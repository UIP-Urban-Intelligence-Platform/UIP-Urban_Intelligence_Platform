---
slug: ai-accident-detection
title: üö® Ph√°t hi·ªán Tai n·∫°n Giao th√¥ng v·ªõi AI
authors: [nguyennhatquang, nguyenviethoang]
tags: [uip, ai, yolox, accident-detection, computer-vision]
---

<!--
SPDX-License-Identifier: MIT
Copyright (c) 2025 UIP Team. All rights reserved.

UIP - Urban Intelligence Platform
Blog post: AI Accident Detection.

Module: apps/traffic-web-app/frontend/docs/blog/2024-05-15-ai-accident-detection.md
Author: UIP Team
Version: 1.0.0
-->

# YOLOX v√† Ph√°t hi·ªán Tai n·∫°n Giao th√¥ng Real-time ü§ñ

M·ªôt trong nh·ªØng t√≠nh nƒÉng quan tr·ªçng nh·∫•t c·ªßa UIP l√† kh·∫£ nƒÉng **ph√°t hi·ªán tai n·∫°n giao th√¥ng t·ª± ƒë·ªông** trong v√≤ng v√†i gi√¢y. B√†i vi·∫øt n√†y chia s·∫ª c√°ch ch√∫ng t√¥i x√¢y d·ª±ng h·ªá th·ªëng n√†y.

<!-- truncate -->

## üéØ V·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt

T·∫°i TP.HCM, m·ªói nƒÉm c√≥ h√†ng ngh√¨n v·ª• tai n·∫°n giao th√¥ng. Th·ªùi gian ph·∫£n ·ª©ng nhanh l√† y·∫øu t·ªë quy·∫øt ƒë·ªãnh ƒë·ªÉ:

- üöë C·ª©u s·ªëng n·∫°n nh√¢n
- üöó Gi·∫£m √πn t·∫Øc giao th√¥ng k√©o d√†i
- üìä Thu th·∫≠p d·ªØ li·ªáu ch√≠nh x√°c

**M·ª•c ti√™u:** Ph√°t hi·ªán tai n·∫°n trong **< 3 gi√¢y** v·ªõi ƒë·ªô ch√≠nh x√°c **> 90%**

## üß† L·ª±a ch·ªçn Model: T·∫°i sao YOLOX?

### So s√°nh c√°c model

| Model | mAP | Speed (FPS) | Size |
|-------|-----|-------------|------|
| YOLOv5 | 50.7% | 140 | 27MB |
| YOLOv7 | 51.2% | 120 | 37MB |
| **YOLOX** | **51.1%** | **155** | **25MB** |
| YOLOR | 52.0% | 80 | 45MB |

**YOLOX** l√† l·ª±a ch·ªçn t·ªëi ∆∞u v√¨:
- ‚úÖ Anchor-free design - ƒë∆°n gi·∫£n h∆°n
- ‚úÖ Decoupled head - train nhanh h∆°n
- ‚úÖ SimOTA label assignment - ch√≠nh x√°c h∆°n
- ‚úÖ Strong augmentation - robust h∆°n

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Camera    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Image     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   YOLOX     ‚îÇ
‚îÇ   Stream    ‚îÇ     ‚îÇ   Buffer    ‚îÇ     ‚îÇ   Model     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Alert     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Post      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Detection  ‚îÇ
‚îÇ   System    ‚îÇ     ‚îÇ   Process   ‚îÇ     ‚îÇ   Results   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üíª Implementation

### 1. Model Loading

```python
import torch
from yolox.exp import get_exp
from yolox.utils import postprocess

class AccidentDetector:
    def __init__(self, model_path: str, device: str = "cuda"):
        self.device = device
        self.exp = get_exp("yolox_s")
        self.model = self.exp.get_model()
        
        # Load pretrained weights
        checkpoint = torch.load(model_path, map_location=device)
        self.model.load_state_dict(checkpoint["model"])
        self.model.to(device)
        self.model.eval()
        
        # Accident-related classes
        self.accident_classes = [
            "car_crash",
            "motorcycle_fallen",
            "person_injured",
            "vehicle_damage"
        ]
```

### 2. Inference Pipeline

```python
async def detect_accidents(self, images: List[np.ndarray]) -> List[Detection]:
    """Process batch of images for accident detection"""
    detections = []
    
    # Preprocess
    batch = self.preprocess_batch(images)
    
    with torch.no_grad():
        # Forward pass
        outputs = self.model(batch)
        
        # Post-process
        outputs = postprocess(
            outputs,
            num_classes=len(self.classes),
            conf_thre=0.7,
            nms_thre=0.45
        )
    
    for i, output in enumerate(outputs):
        if output is None:
            continue
            
        for det in output:
            cls = int(det[6])
            conf = float(det[4] * det[5])
            
            if self.classes[cls] in self.accident_classes:
                detections.append(Detection(
                    image_id=i,
                    class_name=self.classes[cls],
                    confidence=conf,
                    bbox=det[:4].tolist(),
                    timestamp=datetime.now()
                ))
    
    return detections
```

### 3. Alert Dispatch

```python
async def dispatch_alert(self, detection: Detection, camera: Camera):
    """Send accident alert through multiple channels"""
    
    alert = AccidentAlert(
        camera_id=camera.id,
        location=camera.location,
        severity=self.calculate_severity(detection),
        confidence=detection.confidence,
        timestamp=detection.timestamp,
        image_url=await self.capture_snapshot(camera)
    )
    
    # Multi-channel dispatch
    await asyncio.gather(
        self.send_websocket_alert(alert),
        self.send_email_alert(alert),
        self.send_sms_alert(alert),
        self.store_in_database(alert),
        self.publish_ngsi_ld_entity(alert)
    )
```

## üìä Training Data

### Dataset Composition

| Category | Images | Annotations |
|----------|--------|-------------|
| Car crashes | 5,000 | 12,000 |
| Motorcycle falls | 3,500 | 8,000 |
| Pedestrian incidents | 2,000 | 4,500 |
| Multi-vehicle | 1,500 | 5,000 |
| **Total** | **12,000** | **29,500** |

### Data Augmentation

```python
from albumentations import (
    Compose, RandomBrightnessContrast,
    RandomRain, RandomFog, RandomSunFlare,
    HorizontalFlip, RandomScale
)

transform = Compose([
    RandomBrightnessContrast(p=0.5),
    RandomRain(p=0.3),           # M∆∞a
    RandomFog(p=0.2),            # S∆∞∆°ng m√π
    RandomSunFlare(p=0.2),       # Ch√≥i s√°ng
    HorizontalFlip(p=0.5),
    RandomScale(scale_limit=0.2, p=0.5)
])
```

## üìà Performance Results

### Accuracy Metrics

| Metric | Value |
|--------|-------|
| mAP@0.5 | 91.2% |
| mAP@0.5:0.95 | 72.8% |
| Precision | 89.5% |
| Recall | 93.1% |
| F1-Score | 91.3% |

### Speed Metrics

| Hardware | FPS | Latency |
|----------|-----|---------|
| RTX 3080 | 155 | 6.5ms |
| RTX 3060 | 120 | 8.3ms |
| T4 (Cloud) | 95 | 10.5ms |
| CPU (i7-12700) | 15 | 66ms |

## üöÄ Deployment

### GPU Optimization

```python
# TensorRT optimization
import tensorrt as trt

def optimize_for_tensorrt(model_path: str, output_path: str):
    """Convert ONNX model to TensorRT"""
    logger = trt.Logger(trt.Logger.INFO)
    builder = trt.Builder(logger)
    
    config = builder.create_builder_config()
    config.set_flag(trt.BuilderFlag.FP16)  # Half precision
    config.max_workspace_size = 1 << 30    # 1GB
    
    network = builder.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    )
    
    parser = trt.OnnxParser(network, logger)
    with open(model_path, 'rb') as f:
        parser.parse(f.read())
    
    engine = builder.build_serialized_network(network, config)
    
    with open(output_path, 'wb') as f:
        f.write(engine)
```

### Batch Processing

```python
async def process_camera_batch(cameras: List[Camera], batch_size: int = 32):
    """Process cameras in batches for efficiency"""
    
    for i in range(0, len(cameras), batch_size):
        batch = cameras[i:i + batch_size]
        
        # Fetch images concurrently
        images = await asyncio.gather(*[
            fetch_camera_image(cam) for cam in batch
        ])
        
        # Batch inference
        detections = await detector.detect_accidents(images)
        
        # Process detections
        for detection in detections:
            if detection.is_accident:
                await dispatch_alert(detection, batch[detection.image_id])
```

## üìä Real-world Results

Sau 3 th√°ng tri·ªÉn khai th·ª≠ nghi·ªám:

| Metric | Value |
|--------|-------|
| Tai n·∫°n ph√°t hi·ªán | 127 v·ª• |
| Th·ªùi gian ph√°t hi·ªán TB | 2.3 gi√¢y |
| False positives | 8 (6.3%) |
| Th·ªùi gian ph·∫£n ·ª©ng gi·∫£m | 45% |

## üéì Lessons Learned

1. **Lighting matters** - Augmentation v·ªõi c√°c ƒëi·ªÅu ki·ªán √°nh s√°ng kh√°c nhau r·∫•t quan tr·ªçng
2. **Context helps** - K·∫øt h·ª£p th√¥ng tin camera position c·∫£i thi·ªán accuracy
3. **Edge cases** - Xe m√°y nh·ªè kh√≥ ph√°t hi·ªán h∆°n √¥ t√¥
4. **False positives** - C·∫ßn filtering th√¥ng minh ƒë·ªÉ gi·∫£m false alarms

## üîú Future Work

- [ ] Multi-camera tracking
- [ ] Severity estimation
- [ ] Incident type classification
- [ ] Integration v·ªõi emergency services

---

**B·∫°n mu·ªën ƒë√≥ng g√≥p?** Xem [Contributing Guide](/docs/guides/contributing) ƒë·ªÉ b·∫Øt ƒë·∫ßu!

*Nguy·ªÖn Nh·∫≠t Quang & Nguy·ªÖn Vi·ªát Ho√†ng - UIP Team*
